{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from collections import namedtuple\n",
    "import io\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# please check if anyone is using the GPU first before running experim\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if GPU working with tensorflow\n",
    "with tf.Session() as sess:\n",
    "    devices = sess.list_devices()\n",
    "devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data structure\n",
    "!tree -L 4 data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Information\n",
    "\n",
    "From http://multimediaeval.org/mediaeval2018/memorability/index.html\n",
    "\n",
    "## Schedule:\n",
    "+ Data Release: 25 June 2018\n",
    "+ Runs Due: 1 October 2018\n",
    "+ Working Paper Notes Due: 17 October\n",
    "\n",
    "## Task Description\n",
    "+ Automatically predict memorability scores for videos, which reflect the probability of a video being remembered.\n",
    "+ videos with memorability annotations, and pre-extracted state-of-the-art visual features\n",
    "+ The ground truth has been collected through recognition tests\n",
    "+ ‘short-term’ and ‘long-term’ memorability annotations\n",
    "+ Optionally, descriptive titles attached to the videos may be used\n",
    "+ allowed to use external data.\n",
    "\n",
    "## Data\n",
    "+ 10,000 short (soundless) videos extracted from raw footage used by professionals when creating content.\n",
    "\n",
    "Pre extracted features\n",
    "+ HoG descriptors\n",
    "+ LBP\n",
    "+ GIST\n",
    "+ Color Histogram\n",
    "+ Fc7 layer from Inception\n",
    "+ C3D features\n",
    "+ etc\n",
    "\n",
    "## Evaluation\n",
    "+ The outputs of the prediction models – i.e., the predicted memorability scores for the videos – will be compared with ground truth memorability scores using classic evaluation metrics (e.g., Spearman’s rank correlation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Provided Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_set = Path('data/raw/Memorability 2018/dev-set')\n",
    "video_dir = dev_set/'sources'\n",
    "inception_features = dev_set / 'features/InceptionV3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# readme <corrupted somehow>\n",
    "print((dev_set/'README.txt').open('r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# videos\n",
    "video_n = 5\n",
    "\n",
    "videos = sorted((dev_set / 'sources').iterdir())\n",
    "video = videos[video_n].open('r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls width=300>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# video captions\n",
    "video_captions = list((dev_set / 'dev-set_video-captions.txt').open('r'))\n",
    "video_captions = [i.split('\\t') for i in video_captions]\n",
    "video_captions = [[a, b.strip()] for a, b in video_captions]\n",
    "video_captions = pd.DataFrame(video_captions, columns=['video_path', 'caption'])\n",
    "video_captions = video_captions.set_index('video_path')['caption']\n",
    "video_captions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# examine ground truth\n",
    "ground_truth_file = dev_set / 'ground-truth/ground-truth_dev-set.csv'\n",
    "ground_truth = pd.read_csv(ground_truth_file)\n",
    "ground_truth = ground_truth.rename(columns=lambda x: x.replace('-', '_'))\n",
    "ground_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls \"{inception_features}\" | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be three images per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat \"{inception_features}\"/video10-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of key-value pairs, with index between 0-999\n",
    "\n",
    "Values should sum up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_inception_feature(s):\n",
    "    pairs = s.strip().split(' ')\n",
    "    pairs = [i.split(':') for i in pairs]\n",
    "    return {int(k): float(v) for k, v in pairs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inception_feature_file = next(inception_features.iterdir()).open('r').read()\n",
    "sample_inception_feature = parse_inception_feature(inception_feature_file)\n",
    "sample_inception_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets assume that if a key is not present, then it was 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(sample_inception_feature.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sums to *almost* 1\n",
    "\n",
    "Some rounding errors present\n",
    "\n",
    "Finally, we need a way to convert this to a 1000-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_inception_feature(d):\n",
    "    feature = np.zeros(1000)\n",
    "    for k, v in d.items():\n",
    "        feature[k] = v\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_inception_feature2 = expand_inception_feature(sample_inception_feature)\n",
    "sample_inception_feature2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now combine two\n",
    "def parse_and_expand_inception_feature(path):\n",
    "    s = path.open('r').read()\n",
    "    feature = parse_inception_feature(s)\n",
    "    return  expand_inception_feature(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Base line\n",
    "\n",
    "What evaluation metric would we get if we just reported the average memorability, or randomly shuffled them\n",
    "\n",
    "In fact, they organisers weren't very clear what the actual evaluation criteria would be, other than it's to do with sorting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# average short term memorability\n",
    "avg_short_term = ground_truth['short_term_memorability'].mean()\n",
    "avg_long_term = ground_truth['long_term_memorability'].mean()\n",
    "print(\"Avarage short term: {:.4f}\".format(avg_short_term))\n",
    "print(\"Average long term:  {:.4f}\".format(avg_long_term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "trials = 100\n",
    "\n",
    "dummy = []\n",
    "\n",
    "for i in range(trials):\n",
    "    x = np.random.rand(n)\n",
    "    y = ground_truth['short_term_memorability'].sample(n, replace=False).values\n",
    "\n",
    "    dummy += [stats.pearsonr(x, y)]\n",
    "\n",
    "dummy = pd.DataFrame(dummy, columns=['pearson', 'p-value'])\n",
    "dummy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pearson score should be above 0, and consequently have a p-value lower than 0.48\n",
    "\n",
    "Note that there is a high variation, and sometimes the result gives large high pearson correlations and low p-values purely by accident.\n",
    "See next 2 cells.\n",
    "\n",
    "Taking an average over 100 trials seems to give an accurate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standard deviation\n",
    "\n",
    "dummy.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best value in trail\n",
    "\n",
    "dummy.sort_values('pearson', ascending=False).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Attempt: Feed Forward Network from images\n",
    "\n",
    "Strategy: Single dense layer, input image, output is memorability\n",
    "\n",
    "Final output is logistic to force it to be probability\n",
    "\n",
    "Loss function is square loss\n",
    "\n",
    "Metric is 100 examples of 10 videos, and we order them and calculate pearson rank.  \n",
    "We may need to implement this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_inception_fname(fname):\n",
    "    s = str(fname)\n",
    "    match = s.split('-')[-1].split('.')[0]\n",
    "    return int(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = ground_truth.set_index('video').to_dict(orient='index')\n",
    "for video, data in tqdm(dataset.items()):\n",
    "    data['source'] = str(video_dir / video)\n",
    "    glob_string = '{}-*.txt'.format(video.split('.')[0])\n",
    "    inception_files = inception_features.glob(glob_string)\n",
    "    data['inception_features'] = parse_and_expand_inception_feature(\n",
    "        sorted(inception_files)[0])\n",
    "    data['description'] = video_captions.loc[video]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_eval_status(train_ratio=0.5, val_ratio=0.25,\n",
    "                       test_ratio=0.25):\n",
    "    ratio_sum = train_ratio + val_ratio + test_ratio\n",
    "    assert np.isclose(ratio_sum, 1)\n",
    "    \n",
    "    probs = [train_ratio, val_ratio, test_ratio]\n",
    "    choices = ['train', 'val', 'test']\n",
    "    return np.random.choice(choices, p=probs)\n",
    "\n",
    "videos = sorted(dataset.keys())\n",
    "annotations = pd.DataFrame(videos, columns=['video'])\n",
    "annotations['eval_status'] = [choose_eval_status()\n",
    "                              for i in range(len(annotations))]\n",
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = [\"short_term_memorability\", \n",
    "                 \"long_term_memorability\",\n",
    "                 \"inception_features\",\n",
    "                 \"description\"]\n",
    "Feature = namedtuple(\"Feature\", feature_names)\n",
    "\n",
    "def tf_generator(eval_status, shuffle=True):\n",
    "    assert eval_status in ['train', 'val', 'test']\n",
    "    \n",
    "    videos = annotations['video']\n",
    "    videos = videos[annotations['eval_status']==eval_status].values\n",
    "    \n",
    "    def f():\n",
    "        if shuffle:\n",
    "            random.shuffle(videos)\n",
    "        for video in videos:\n",
    "            feature_dict = {feature: dataset[video][feature] \n",
    "                            for feature in feature_names}\n",
    "            yield Feature(**feature_dict)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_features(short_term_memorability, long_term_memorability,\n",
    "                     inception_features, description):\n",
    "    return short_term_memorability, inception_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data_init_op(eval_mode, batch_size, iterator, shuffle=True):\n",
    "    dataset = tf_generator(eval_mode, shuffle)\n",
    "    output_types = (tf.float32, tf.float32, tf.float32, tf.string)\n",
    "    dataset = tf.data.Dataset.from_generator(dataset,\n",
    "                                             output_types=output_types)\n",
    "    dataset = dataset.map(process_features).batch(batch_size).prefetch(1)\n",
    "    return iterator.make_initializer(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_pipeline(batch_size):\n",
    "    input_types = (tf.float32, tf.float32)\n",
    "    input_shapes = (tf.TensorShape(None),\n",
    "                    tf.TensorShape([None, tf.Dimension(1000)]))\n",
    "    iterator = tf.data.Iterator.from_structure(input_types, \n",
    "                                               input_shapes)\n",
    "    memorability, inception_features = iterator.get_next()\n",
    "    init_train_pipeline_op = create_data_init_op('train', batch_size, \n",
    "                                                 iterator)\n",
    "    init_val_pipeline_op = create_data_init_op('val', batch_size,\n",
    "                                               iterator)\n",
    "    init_test_pipeline_op = create_data_init_op('test', batch_size,\n",
    "                                                iterator)\n",
    "    return ((memorability, inception_features), \n",
    "            (init_train_pipeline_op, init_val_pipeline_op,\n",
    "             init_test_pipeline_op))\n",
    "\n",
    "((memorability, inception_features),\n",
    " (train_pipeline, val_pipeline, test_pipeline)) = create_pipeline(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense = tf.layers.dense(inception_features, units=1)\n",
    "loss = tf.losses.mean_squared_error(labels=memorability,\n",
    "                                    predictions=dense)\n",
    "\n",
    "mean_loss_op, mean_loss_update_op = tf.metrics.mean(loss, name='metric')\n",
    "running_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES,\n",
    "                                         scope=\"metric\")\n",
    "reset_metrics_op = tf.variables_initializer(var_list=running_vars)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-3)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth['short_term_memorability'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(val_pipeline)\n",
    "\n",
    "# original val loss\n",
    "sess.run(reset_metrics_op)\n",
    "while True:\n",
    "    try:\n",
    "        sess.run(mean_loss_update_op)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "print(\"Original validation loss\")\n",
    "print(sess.run(mean_loss_op))\n",
    "\n",
    "# train for one epoch\n",
    "for i in range(100):\n",
    "    sess.run(train_pipeline)\n",
    "    while True:\n",
    "        try:\n",
    "            sess.run(train_op)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "# loss after training\n",
    "sess.run(reset_metrics_op)\n",
    "sess.run(val_pipeline)\n",
    "while True:\n",
    "    try:\n",
    "        sess.run(mean_loss_update_op)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "print(\"Loss after one hundred epochs\")\n",
    "print(sess.run(mean_loss_op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test ranking error\n",
    "sess.run(test_pipeline)\n",
    "predictions = []\n",
    "while True:\n",
    "    try:\n",
    "        predictions += [sess.run(dense)]\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = [item.item() for sublist in predictions for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_idx = annotations[annotations['eval_status']=='test'].index\n",
    "test_ground_truth = ground_truth.loc[test_idx]['short_term_memorability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats.pearsonr(test_ground_truth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data=[test_ground_truth.values, predictions],\n",
    "             index=['ground truth', 'predictions']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do smarter things than this, need to read the literature first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "http://multimediaeval.org/mediaeval2018/memorability/index.html\n",
    "\n",
    "MediaEval Working Notes 2018 - Google Doc\n",
    "\n",
    "Recommended Papers:\n",
    "1. Aditya Khosla, Akhil S Raju, Antonio Torralba, and Aude Oliva. 2015. [Understanding and predicting image memorability at a large scale](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Khosla_Understanding_and_Predicting_ICCV_2015_paper.pdf). In Proc. IEEE Int. Conf. on Computer Vision (ICCV). 2390–2398.\n",
    "2. Phillip Isola, Jianxiong Xiao, Devi Parikh, Antonio Torralba, and Aude Oliva. 2014. [What makes a photograph memorable?](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6629991) IEEE Transactions on Pattern Analysis and Machine Intelligence 36, 7 (2014), 1469–1482.\n",
    "3. Hammad Squalli-Houssaini, Ngoc Duong, Marquant Gwenaëlle, and Claire-Hélène Demarty. 2018. [Deep learning for predicting image memorability](https://hal.archives-ouvertes.fr/hal-01629297/document). In Proc. IEEE Int. Conf. on Audio, Speech and Language Processing (ICASSP).\n",
    "4. Junwei Han, Changyuan Chen, Ling Shao, Xintao Hu, Jungong Han, and Tianming Liu. 2015. [Learning computational models of video memorability from fMRI brain imaging](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6919270). IEEE transactions on cybernetics 45, 8 (2015), 1692–1703.\n",
    "5. Sumit Shekhar, Dhruv Singal, Harvineet Singh, Manav Kedia, and Akhil Shetty. 2017. [Show and Recall: Learning What Makes Videos Memorable].(http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w40/Shekhar_Show_and_Recall_ICCV_2017_paper.pdf) In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2730–2739.\n",
    "6. Romain Cohendet, Karthik Yadati, Ngoc K.Q. Duong and Claire-Hélène Demarty. 2018. [Annotating, Understanding, and Predicting Long-term Video Memorability](https://sci-hub.tw/10.1145/3206025.3206056)\n",
    ". In Proceedings of the ACM International Conference on Multimedia Retrieval (ICMR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Evaluation Updates\n",
    "\n",
    "Results were pretty poor compared to other entries, even other entries doing simple analysis.\n",
    "\n",
    "Accoriding to [winning paper](http://ceur-ws.org/Vol-2283/MediaEval_18_paper_31.pdf), we should have been able to achieve spearman correlation of 0.092 using just inception features. However we achieved -0.017.\n",
    "\n",
    "I suspect that the logistic regression was a poor choice.\n",
    "\n",
    "Lets try again, a few more things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1: Improve code\n",
    "\n",
    "Here we fix the softmax code I was using, to make sure it's working correctly.\n",
    "Note that the labels are not 1, 0 but probalities. Hence we need to add 1-memorability to ensure cross entropy works correctly.\n",
    "\n",
    "Add an evaluate function for easier evaluation.\n",
    "\n",
    "Also we forgot to use a softmax to generate predictions\n",
    "\n",
    "Was calculating pearson r in some cases, instead of spearman correlation by mistake.\n",
    "\n",
    "Also changed spearman correlation to gaurantee that predictions and labels are from the same rows. I don't think I was making this mistake, but I've changed the code to make sure this cannot happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will reuse the tensorflow variables created above, but create a new session\n",
    "sess.close()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reuse memorability, inception_features, train_pipeline, val_pipeline, test_pipeline variables\n",
    "# note: tensorflow is probabily overkill for this, but we can use the same template when we start analysing videos\n",
    "\n",
    "# model\n",
    "dense = tf.layers.dense(inception_features, units=2)\n",
    "preds = tf.nn.softmax(dense)[:, 1]\n",
    "labels = tf.transpose(tf.convert_to_tensor([memorability, 1-memorability]))\n",
    "errors = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=dense))\n",
    "loss = tf.reduce_mean(errors)\n",
    "\n",
    "# metrics\n",
    "running_loss, running_loss_update = tf.metrics.mean(errors, name=\"metric\")\n",
    "running_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"metric\")\n",
    "reset_metrics_op = tf.variables_initializer(var_list=running_vars)\n",
    "\n",
    "# optimizers\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-3)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "examine variables from this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(train_pipeline)\n",
    "\n",
    "(sample_memorability,\n",
    " sample_labels,\n",
    " sample_prediction,\n",
    " sample_loss) = sess.run([memorability, labels, preds, loss])\n",
    "\n",
    "# reset session for training\n",
    "sess.close()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_memorability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_prediction[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to evalutate model\n",
    "\n",
    "def evaluate(preds, running_loss, running_loss_update, reset_metrics_op,\n",
    "             memorability=memorability, train_pipeline=train_pipeline, test_pipeline=test_pipeline):\n",
    "    # evaluate on training dataset\n",
    "    sess.run(reset_metrics_op)\n",
    "    sess.run(train_pipeline)\n",
    "    train_predictions, train_labels = [], [] \n",
    "    while True:\n",
    "        try:\n",
    "            _, a, b = sess.run([running_loss_update, preds, memorability])\n",
    "            train_predictions = np.append(train_predictions, a)\n",
    "            train_labels = np.append(train_labels, b)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    train_xent = sess.run(running_loss)\n",
    "    results = pd.DataFrame([train_predictions, train_labels], index=['memorability', 'pred']).T\n",
    "    train_spearman = results.corr(method='spearman').iloc[0, 1]\n",
    "\n",
    "    # evaluate on test dataset\n",
    "    sess.run(reset_metrics_op)\n",
    "    sess.run(test_pipeline)\n",
    "    test_predictions, test_labels = [], [] \n",
    "    while True:\n",
    "        try:\n",
    "            _, a, b = sess.run([running_loss_update, preds, memorability])\n",
    "            test_predictions = np.append(test_predictions, a)\n",
    "            test_labels = np.append(test_labels, b)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    test_xent = sess.run(running_loss)\n",
    "    results = pd.DataFrame([test_predictions, test_labels], index=['memorability', 'pred']).T\n",
    "    test_spearman = results.corr(method='spearman').iloc[0, 1]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'xent': {'test': test_xent, 'train': train_xent},\n",
    "        'spearman': {'test': test_spearman, 'train': train_spearman}\n",
    "    }).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(test_pipeline)\n",
    "\n",
    "evaluate(preds, running_loss, running_loss_update, reset_metrics_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train for one epoch\n",
    "for i in range(1):\n",
    "    sess.run(train_pipeline)\n",
    "    while True:\n",
    "        try:\n",
    "            sess.run(train_op)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "sess.run(reset_metrics_op)\n",
    "sess.run(val_pipeline)\n",
    "while True:\n",
    "    try:\n",
    "        sess.run(running_loss_update)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "\n",
    "evaluate(preds, running_loss, running_loss_update, reset_metrics_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train for 100 epochs\n",
    "for i in tqdm(range(99)):\n",
    "    sess.run(train_pipeline)\n",
    "    while True:\n",
    "        try:\n",
    "            sess.run(train_op)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "evaluate(preds, running_loss, running_loss_update, reset_metrics_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginal improvement in loss, train and test loss essentially the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train for 1000 epochs\n",
    "for i in tqdm(range(900)):\n",
    "    sess.run(train_pipeline)\n",
    "    while True:\n",
    "        try:\n",
    "            sess.run(train_op)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "evaluate(preds, running_loss, running_loss_update, reset_metrics_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems we are not able to make the algotihm overfit\n",
    "\n",
    "I should have spent more time here in retrospect, before moving on\n",
    "\n",
    "Lets keep iterating until we can get this linear model to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2: Try square loss instead of softmax\n",
    "\n",
    "In the winning paper, they used LASSO L1 regularized regression.\n",
    "\n",
    "Lets first use unregularised regression and demonstrate that it can overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will reuse the tensorflow variables created above, but create a new session\n",
    "sess.close()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reuse memorability, inception_features, train_pipeline, val_pipeline, test_pipeline variables\n",
    "# note: tensorflow is probabily overkill for this, but we can use the same template when we start analysing videos\n",
    "\n",
    "# model\n",
    "dense = tf.layers.dense(inception_features, units=1)\n",
    "preds = tf.nn.sigmoid(dense)\n",
    "loss = tf.losses.mean_squared_error(labels=memorability, predictions=preds)\n",
    "\n",
    "# metrics\n",
    "running_loss, running_loss_update = tf.metrics.mean(loss, name=\"metric\")\n",
    "running_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"metric\")\n",
    "reset_metrics_op = tf.variables_initializer(var_list=running_vars)\n",
    "\n",
    "# optimizers\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-3)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(test_pipeline)\n",
    "\n",
    "# loss before training\n",
    "evaluate(preds, running_loss, running_loss_update, reset_metrics_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train for one epoch\n",
    "for i in range(1):\n",
    "    sess.run(train_pipeline)\n",
    "    while True:\n",
    "        try:\n",
    "            sess.run(train_op)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "sess.run(reset_metrics_op)\n",
    "sess.run(val_pipeline)\n",
    "while True:\n",
    "    try:\n",
    "        sess.run(running_loss_update)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "\n",
    "evaluate(preds, running_loss, running_loss_update, reset_metrics_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is only marginally smaller, spearman is still negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train for 100 epochs\n",
    "for i in tqdm(range(99)):\n",
    "    sess.run(train_pipeline)\n",
    "    while True:\n",
    "        try:\n",
    "            sess.run(train_op)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "evaluate(preds, running_loss, running_loss_update, reset_metrics_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss have reduced significantly, spearman is steadily improving\n",
    "\n",
    "Looks like we might finally be seeing some small overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train for 1,000 epochs\n",
    "for i in tqdm(range(900)):\n",
    "    sess.run(train_pipeline)\n",
    "    while True:\n",
    "        try:\n",
    "            sess.run(train_op)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "evaluate(preds, running_loss, running_loss_update, reset_metrics_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant improvement in cross entropy, but no corresponding improvement in spearman correlation.\n",
    "Seems to have some slight overfitting, which is positive, but it is too small to tell for certain.\n",
    "\n",
    "Finally seem to be getting some reasonable results for spearman correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo**\n",
    "\n",
    "This was pretty similar to one of our [submitted approached](https://gitlab.insight-centre.org/owen.corrigan/memorability/blob/master/src/models/inception_model.py).\n",
    "\n",
    "So why didn't the other one work? Will have to take a look at this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 3: Z-score Label normalization\n",
    "\n",
    "According to http://ceur-ws.org/Vol-2283/MediaEval_18_paper_31.pdf, they used z-score normalization when training regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate mean and std devaiation of test set.\n",
    "# apply same values when doing z score normalization of test and val sets\n",
    "\n",
    "train_videos = annotations[annotations['eval_status']=='train']['video'].values\n",
    "train_memorability = ground_truth.set_index('video').loc[train_videos]['short_term_memorability']\n",
    "mean, std = train_memorability.mean(), train_memorability.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will reuse the tensorflow variables created above, but create a new session\n",
    "sess.close()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "dense = tf.layers.dense(inception_features, units=1)[:, 0]\n",
    "preds = dense\n",
    "memorability_scaled = (memorability - mean) / std\n",
    "loss = tf.losses.mean_squared_error(labels=memorability_scaled, predictions=dense)\n",
    "\n",
    "# metrics\n",
    "running_loss, running_loss_update = tf.metrics.mean(loss, name=\"metric\")\n",
    "running_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"metric\")\n",
    "reset_metrics_op = tf.variables_initializer(var_list=running_vars)\n",
    "\n",
    "# optimizers\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-3)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "examine variables from this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(test_pipeline)\n",
    "sample_preds, sample_memorability = sess.run([preds, memorability_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_memorability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(test_pipeline)\n",
    "\n",
    "# loss before training\n",
    "evaluate(preds, running_loss, running_loss_update, reset_metrics_op,\n",
    "         memorability=memorability_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy / KL Divergence is 10 times higher before being trained.\n",
    "\n",
    "Maybe this gives more opportunity to learn somehow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train for one epoch\n",
    "for i in range(1):\n",
    "    sess.run(train_pipeline)\n",
    "    while True:\n",
    "        try:\n",
    "            sess.run(train_op)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "sess.run(reset_metrics_op)\n",
    "sess.run(val_pipeline)\n",
    "while True:\n",
    "    try:\n",
    "        sess.run(running_loss_update)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "\n",
    "evaluate(preds, running_loss, running_loss_update, reset_metrics_op,\n",
    "         memorability=memorability_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train for 100 epochs\n",
    "for i in tqdm(range(99)):\n",
    "    sess.run(train_pipeline)\n",
    "    while True:\n",
    "        try:\n",
    "            sess.run(train_op)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "evaluate(preds, running_loss, running_loss_update, reset_metrics_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seeing small overfitting, plus good spearman results.\n",
    "\n",
    "Good sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train for 1,000 epochs\n",
    "for i in tqdm(range(900)):\n",
    "    sess.run(train_pipeline)\n",
    "    while True:\n",
    "        try:\n",
    "            sess.run(train_op)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "evaluate(preds, running_loss, running_loss_update, reset_metrics_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we are starting to see some more overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is model predicting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(test_pipeline)\n",
    "(sample_preds,\n",
    " sample_memorability,\n",
    " sample_memorability_scaled) = sess.run([preds, memorability, memorability_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are new scaled values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_memorability_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that memorability may now be negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 4: Beta Distribution\n",
    "\n",
    "[This comment](https://www.reddit.com/r/MachineLearning/comments/9tptih/d_what_loss_function_to_use_for_probability/e8yyy7d) had an interesting idea\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 5: Sampling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
